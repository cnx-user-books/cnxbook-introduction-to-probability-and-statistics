<document xmlns="http://cnx.rice.edu/cnxml" xmlns:m="http://www.w3.org/1998/Math/MathML">
  <title>The Regression Equation</title>
  <metadata xmlns:md="http://cnx.rice.edu/mdml">
  <md:content-id>m62444</md:content-id>
  <md:title>The Regression Equation</md:title>
  <md:abstract/>
  <md:uuid>3ceae522-0d0f-438c-91c6-5c3aec6b7277</md:uuid>
</metadata>

<content>
    <para id="delete_me">Data rarely fit a straight line exactly. Usually, you must be satisfied with rough
predictions. Typically, you have a set of data whose scatter plot appears to <emphasis>"fit"</emphasis> a
straight line. This straight line is called the <term>Line of Best Fit or Least Squares Regression Line</term>.
<newline/></para><section id="element-748"><title>Optional Collaborative Classroom Activity</title>
<para id="element-900">If you know a person's pinky (smallest) finger length, do you think you could predict that
person's height? Collect data from your class (pinky finger length, in inches). The
independent variable, <m:math><m:mi>x</m:mi></m:math>, is pinky finger length and the dependent variable, <m:math><m:mi>y</m:mi></m:math>, is height.
<newline/><newline/></para><para id="element-657">For each set of data, plot the points on graph paper. Make your graph big enough and
<emphasis>use a ruler</emphasis>. Then "by eye" draw a line that appears to "fit" the data. For your line, pick
two convenient points and use them to find the slope of the line. Find the <m:math><m:mi>y</m:mi></m:math>-intercept of
the line by extending your lines so they cross the <m:math><m:mi>y</m:mi></m:math>-axis. Using the slopes and the
<m:math><m:mi>y</m:mi></m:math>-intercepts, write your equation of "best fit". Do you think everyone will have the same
equation? Why or why not?
<newline/><newline/></para><para id="element-598">Using your equation, what is the predicted height for a pinky length of 2.5 inches?
<newline/></para></section><example id="element-22"><para id="element-998">A random sample of 11 statistics students produced the following data
where <m:math><m:mi>x</m:mi></m:math> is the third exam score, out of 80, and <m:math><m:mi>y</m:mi></m:math> is the final exam score, out of 200.
Can you predict the final exam score of a random student if you know the third exam score?
<newline/></para><figure id="linrgs_regeq1"><subfigure id="id8083493">

<table id="element-50" summary="">
<tgroup cols="2"><thead>
  <row>
    <entry>x (third exam score)</entry>
    <entry>y (final exam score)</entry>
  </row>
</thead>
<tbody>
  <row>
    <entry>65</entry>
    <entry>175</entry>
  </row>
  <row>
    <entry>67</entry>
    <entry>133</entry>
  </row>
  <row>
    <entry>71</entry>
    <entry>185</entry>
  </row>
  <row>
    <entry>71</entry>
    <entry>163</entry>
  </row>
  <row>
    <entry>66</entry>
    <entry>126</entry>
  </row>
  <row>
    <entry>75</entry>
    <entry>198</entry>
  </row>
  <row>
    <entry>67</entry>
    <entry>153</entry>
  </row>
  <row>
    <entry>70</entry>
    <entry>163</entry>
  </row>
  <row>
    <entry>71</entry>
    <entry>159</entry>
  </row>
  <row>
    <entry>69</entry>
    <entry>151</entry>
  </row>
  <row>
    <entry>69</entry>
    <entry>159</entry>
  </row>
</tbody>

</tgroup>
</table>
<caption>Table showing the scores on the final exam based on scores from the third exam.</caption>
</subfigure>
<subfigure id="id7881358">
<media id="id1164262330756" alt="Scatterplot of exam scores with the third exam score on the x-axis and the final exam score on the y-axis."><image src="../../media/linrgs_regeq1.png" mime-type="image/png" print-width="3in"/></media>
<caption>Scatterplot showing the scores on the final exam based on scores from the third exam.</caption>
</subfigure></figure></example><para id="eip-892">The third exam score, <m:math><m:mi>x</m:mi></m:math>, is the independent variable and the final exam score, <m:math><m:mi>y</m:mi></m:math>, is the
dependent variable. We will plot a regression line that best "fits" the data. If each of you
were to fit a line "by eye", you would draw different lines. We can use what is called a
<term>least-squares regression line</term> to obtain the best fit line.
<newline/><newline/></para><para id="element-644">Consider the following diagram. Each point of data is of the the form <m:math><m:mo>(</m:mo><m:mi>x</m:mi><m:mo>,</m:mo><m:mi>y</m:mi><m:mo>)</m:mo></m:math> and each point of
the line of best fit using least-squares linear regression has the form

<m:math>
<m:mo>(</m:mo>
<m:mi>x</m:mi>
<m:mo>,</m:mo>
<m:mover>
<m:mi>y</m:mi>
<m:mo>^</m:mo>
</m:mover>
<m:mo>)</m:mo>
</m:math>.
<newline/><newline/></para><para id="element-51">The <m:math><m:mover><m:mi>y</m:mi><m:mo>^</m:mo></m:mover></m:math> is read <emphasis>"y hat"</emphasis> and is the <emphasis>estimated value of <m:math><m:mi>y</m:mi></m:math></emphasis>. It is the value of <m:math><m:mi>y</m:mi></m:math> obtained using the
regression line. It is not generally equal to <m:math><m:mi>y</m:mi></m:math> from data.
<newline/></para><para id="element-530"><figure id="linrgs_regeq2"><media id="id1164271221679" alt="Scatterplot of the exam scores with a line of best fit tying in the relationship between the third exam and final exam scores. A specific point on the line, specific data point, and the distance between these two points are used in order to show an example of how to compute the sum of squared errors in order to find the points on the line of best fit."><image src="../../media/linrgs_regeq2.png" mime-type="image/png" print-width="4.5in" width="480"/></media></figure></para><para id="element-621">The term <m:math><m:msub><m:mi>y</m:mi><m:mn>0</m:mn></m:msub><m:mo>-</m:mo><m:msub><m:mover><m:mi>y</m:mi><m:mo>^</m:mo></m:mover><m:mn>0</m:mn></m:msub><m:mo>=</m:mo><m:msub><m:mi>ε</m:mi><m:mn>0</m:mn></m:msub></m:math> is called the <emphasis>"error" or residual</emphasis>. It is not an error in the
sense of a mistake. The <emphasis>absolute value of a residual</emphasis> measures the vertical distance between the actual value of <m:math><m:mi>y</m:mi></m:math> and the
estimated value of <m:math><m:mi>y</m:mi></m:math>.
In other words, it measures the vertical distance between the actual data point and the predicted point on the line.
<newline/><newline/></para><para id="eip-344"><m:math><m:mi>ε</m:mi></m:math> = the Greek letter <emphasis>epsilon</emphasis>
<newline/><newline/></para><para id="eip-462">If the observed data point lies above the line, the residual is positive, and the line underestimates the actual data value for <m:math> <m:mi>y</m:mi> </m:math>.  If the observed data point lies below the line, the residual is negative, and the line overestimates that actual data value for <m:math> <m:mi>y</m:mi> </m:math>.
<newline/><newline/></para><para id="eip-104">In the diagram above, <m:math><m:msub><m:mi>y</m:mi><m:mn>0</m:mn></m:msub><m:mo>-</m:mo><m:msub><m:mover><m:mi>y</m:mi><m:mo>^</m:mo></m:mover><m:mn>0</m:mn></m:msub><m:mo>=</m:mo><m:msub><m:mi>ε</m:mi><m:mn>0</m:mn></m:msub></m:math> is the residual for the point shown.  Here the point lies above the line and the residual is positive.
<newline/><newline/></para><para id="element-15">For each data point, you can calculate the residuals or errors, <m:math><m:msub><m:mi>y</m:mi><m:mi>i</m:mi></m:msub><m:mo>-</m:mo><m:msub><m:mover><m:mi>y</m:mi><m:mo>^</m:mo></m:mover><m:mi>i</m:mi></m:msub><m:mo>=</m:mo><m:msub><m:mi>ε</m:mi><m:mi>i</m:mi></m:msub></m:math> for <m:math><m:mi>i</m:mi><m:mo>=</m:mo><m:mtext>1, 2, 3, ..., 11</m:mtext></m:math>.
<newline/><newline/></para><para id="element-670">Each <m:math><m:mo>|</m:mo><m:mi>ε</m:mi><m:mo>|</m:mo></m:math> is a vertical distance.
<newline/><newline/></para><para id="element-610">For the example about the third exam scores and the final exam scores for the 11
statistics students, there are 11 data points. Therefore, there are 11 <m:math><m:mi>ε</m:mi></m:math> values. If you
square each <m:math><m:mi>ε</m:mi></m:math> and add, you get

<newline/><newline/></para><para id="element-575"><m:math>
<m:mo>(</m:mo>
<m:msub>
<m:mi>ε</m:mi>
<m:mn>1</m:mn>
</m:msub>
<m:msup>
<m:mo>)</m:mo>
<m:mn>2</m:mn>
</m:msup>
<m:mo>+</m:mo>
<m:mo>(</m:mo>
<m:msub>
<m:mi>ε</m:mi>
<m:mn>2</m:mn>
</m:msub>
<m:msup>
<m:mo>)</m:mo>
<m:mn>2</m:mn>
</m:msup>
<m:mo>+</m:mo>
<m:mtext>...</m:mtext>
<m:mo>+</m:mo>
<m:mo>(</m:mo>
<m:msub>
<m:mi>ε</m:mi>
<m:mn>11</m:mn>
</m:msub>
<m:msup>
<m:mo>)</m:mo>
<m:mn>2</m:mn>
</m:msup>
<m:mo>=</m:mo>
<m:mover>
<m:mrow>
<m:munder>
<m:mi>Σ</m:mi>
<m:mtext>i = 1</m:mtext>
</m:munder>
</m:mrow>
<m:mn>11</m:mn>
</m:mover>
<m:msup>
<m:mi>ε</m:mi>
<m:mn>2</m:mn>
</m:msup>
</m:math>
<newline/><newline/></para><para id="element-215">This is called the <emphasis>Sum of Squared Errors (SSE)</emphasis>.
<newline/><newline/></para><para id="element-640">Using calculus, you can determine the values of <m:math><m:mi>a</m:mi></m:math> and <m:math><m:mi>b</m:mi></m:math> that make the <emphasis>SSE</emphasis> a minimum. When you make the <emphasis>SSE</emphasis> a
minimum, you have determined the points that are on the line of best fit. It turns out that
the line of best fit has the equation:
<newline/><newline/></para><equation id="element-710"><m:math>
<m:mover>
<m:mi>y</m:mi>
<m:mo>^</m:mo>
</m:mover>
<m:mo>=</m:mo>
<m:mi>a</m:mi>
<m:mo>+</m:mo>
<m:mi>b</m:mi><m:mi>x</m:mi>
</m:math>
</equation><para id="element-716">where <m:math>
<m:mi>a</m:mi><m:mo>=</m:mo><m:mover><m:mi>y</m:mi><m:mi>¯</m:mi></m:mover><m:mo>-</m:mo><m:mi>b</m:mi><m:mo>⋅</m:mo><m:mover><m:mi>x</m:mi><m:mi>¯</m:mi></m:mover></m:math>
and 

<m:math>
<m:mi>b</m:mi><m:mo>=</m:mo>

<m:mfrac>
<m:mrow>
<m:mi>Σ</m:mi>
<m:mo>(</m:mo>
<m:mi>x</m:mi>
<m:mo>-</m:mo>
<m:mover>
<m:mi>x</m:mi>
<m:mi>¯</m:mi>
</m:mover>
<m:mo>)</m:mo>
<m:mo>⋅</m:mo>
<m:mo>(</m:mo>
<m:mi>y</m:mi>
<m:mo>-</m:mo>
<m:mover>
<m:mi>y</m:mi>
<m:mi>¯</m:mi>
</m:mover>
<m:mo>)</m:mo>
</m:mrow>

<m:mrow>
<m:msup>
<m:mrow>
<m:mi>Σ</m:mi>
<m:mo>(</m:mo>
<m:mi>x</m:mi>
<m:mo>-</m:mo>
<m:mover>
<m:mi>x</m:mi>
<m:mi>¯</m:mi>
</m:mover>
<m:mo>)</m:mo>
</m:mrow>
<m:mrow>
<m:mn>2</m:mn>
</m:mrow>
</m:msup>
</m:mrow>
</m:mfrac>
</m:math>.
<newline/><newline/></para><para id="element-153"><m:math><m:mover><m:mi>x</m:mi><m:mi>¯</m:mi></m:mover></m:math> and <m:math><m:mover><m:mi>y</m:mi><m:mi>¯</m:mi></m:mover></m:math> are the sample means of the <m:math><m:mi>x</m:mi></m:math> values and the <m:math><m:mi>y</m:mi></m:math> values, respectively. The best fit line always passes through the point
<m:math><m:mo>(</m:mo><m:mover><m:mi>x</m:mi><m:mi>¯</m:mi></m:mover><m:mo>,</m:mo><m:mover><m:mi>y</m:mi><m:mi>¯</m:mi></m:mover><m:mo>)</m:mo></m:math>.
<newline/><newline/></para><para id="element-414">The slope <m:math><m:mi>b</m:mi></m:math> can be written as  
<m:math><m:mi>b</m:mi><m:mo>=</m:mo><m:mi>r</m:mi><m:mo>⋅</m:mo><m:mo>(</m:mo>
<m:mfrac><m:mrow>
<m:msub><m:mi>s</m:mi><m:mi>y</m:mi></m:msub></m:mrow>
<m:mrow>
<m:msub><m:mi>s</m:mi><m:mi>x</m:mi></m:msub></m:mrow></m:mfrac><m:mo>)</m:mo></m:math> where <m:math><m:msub><m:mi>s</m:mi><m:mi>y</m:mi></m:msub></m:math>
= the standard deviation of the
<m:math><m:mi>y</m:mi></m:math> values and <m:math><m:msub><m:mi>s</m:mi><m:mi>x</m:mi></m:msub></m:math> = the standard deviation of the <m:math><m:mi>x</m:mi></m:math> values. <m:math><m:mi>r</m:mi></m:math> is the correlation
coefficient which is discussed in the next section.
<newline/><newline/></para><para id="eip-996"><title>Least Squares Criteria for Best Fit</title>The process of fitting the best fit line is called <term>linear regression</term>. The idea behind finding the best fit line is based on the assumption that the data are scattered about a straight line. The criteria for the best fit line is that the sum of the squared errors (SSE) is minimized, that is made as small as possible. Any other line you might choose would have a higher SSE than the best fit line. This best fit line is called the <term> least squares regression line </term>.
<newline/><newline/></para><note id="id1164273503037">Computer spreadsheets, statistical software, and many calculators can quickly
calculate the best fit line and create the graphs. The calculations tend to be tedious if done by hand. Instructions to use the TI-83, TI-83+, and TI-84+ calculators to find the best fit line and create a scatterplot are shown at the end of this section. 
<newline/></note><para id="element-27"><title>THIRD EXAM vs FINAL EXAM EXAMPLE:</title>The graph of the line of best fit for the third exam/final exam example is shown below:
<newline/></para><figure id="linrgs_regeq3"><media id="id1164250764889" alt="Scatterplot of the third exam scores by final exam scores and its line of best fit."><image src="../../media/linrgs_regeq3.png" mime-type="image/png" print-width="3in" width="380"/></media></figure><para id="element-689">The least squares regression line (best fit line) for the third exam/final exam example has the equation:
<newline/></para><equation id="element-643"><m:math>
<m:mover>
<m:mi>y</m:mi>
<m:mo>^</m:mo>
</m:mover>
<m:mo>=</m:mo>
<m:mo>-</m:mo><m:mn>173.51</m:mn>
<m:mo>+</m:mo>
<m:mn>4.83</m:mn><m:mi>x</m:mi>
<m:mspace width="20pt"/>
</m:math></equation><note id="eip-923"><list id="id150181954" list-type="bulleted" bullet-style="bullet"><item>Remember, it is always important to plot a
scatterplot first. If the scatterplot indicates that there is a linear relationship between
the variables, then it is reasonable to use a best fit line to make predictions for
<m:math><m:mi>y</m:mi></m:math> given <m:math><m:mi>x</m:mi></m:math> within the domain of <m:math><m:mi>x</m:mi></m:math>-values in the sample data, <emphasis>but not necessarily
for <m:math><m:mi>x</m:mi></m:math>-values outside that domain.</emphasis></item> <item> You could use the line to predict the final exam score for a student who earned a grade of 73 on the third exam.</item>  <item>You should NOT use the line to predict the final exam score for a student who earned a grade of 50 on the third exam, because 50 is not within the domain of the <m:math><m:mi>x</m:mi></m:math>-values in the sample data, which are between 65 and 75.<newline/> </item>
</list></note><para id="element-39"><title>UNDERSTANDING SLOPE</title>The slope of the line, <m:math><m:mi>b</m:mi></m:math>, describes how changes in the variables are related. It is important to interpret the slope of the line in the context of the situation represented by the data. You should be able to write a sentence interpreting the slope in plain English.
<newline/><newline/></para><para id="element-725"><emphasis>INTERPRETATION OF THE SLOPE:</emphasis> The slope of the best fit line tells us how the dependent variable (<m:math><m:mi>y</m:mi></m:math>) changes for every one unit increase in the independent (<m:math><m:mi>x</m:mi></m:math>) variable, on average. 
<newline/><newline/></para><list id="eip-553" list-type="labeled-item"><title>THIRD EXAM vs FINAL EXAM EXAMPLE</title><item>Slope: The slope of the line is <m:math><m:mi>b</m:mi></m:math> = 4.83. </item>
<item>Interpretation:  For a one point increase in the score on the third exam, the final exam score increases by 4.83 points, on average.
<newline/></item>
</list><section id="eip-349"><title>Using the TI-83+ and TI-84+ Calculators</title><list id="eip-480" list-type="enumerated" number-style="arabic" class="stepwise"><title>Using the calculator to find the linear regression equation</title><item> Turn on the diagnostics, by hitting 2nd Catalog (under the number 0), scroll to "Diagnostic On", hit enter and then enter again. (You only need to do this step once.)</item>
<item> In the STAT list editor, enter the X data in list L1 and the Y data in list L2, paired so that the corresponding (x,y) values are next to each other in the lists. (If a particular pair of values is repeated, enter it as many times as it appears in the data.)</item>
<item> On the STAT CALC menu, scroll down with the cursor to select the #8:LinReg(a+bx) and hit enter.</item> 
<item> Hit enter one more time (or enter the appropriate list names, if requested, and then hit enter).</item></list><para id="element-5301"><figure id="linregttestscreens"><media id="id53013501" alt="1. Image of calculator input screen for LinRegTTest with input matching the instructions above. 2.Image of corresponding output calculator output screen for LinRegTTest: Output screen shows: Line 1. LinRegTTest; Line 2. y = a + bx; Line 3. beta does not equal 0 and rho does not equal 0; Line 4. t = 2.657560155; Line 5. df = 9; Line 6. a = 173.513363; Line 7. b = 4.827394209; Line 8. s = 16.41237711; Line 9. r squared = .4396931104; Line 10. r = .663093591"><image src="../../media/LinReg1.png" mime-type="image/png" print-width="2in" width="180"/></media></figure></para>
<list id="eip-720" list-type="labeled-item"><item>The first line says y=a+bx. Scroll down to find the values a=-173.513363, and b=4.827394209 ; rounding each constant to two decimal places, the equation of the linear regression (best fit) line is <m:math>
<m:mover>
<m:mi>y</m:mi>
<m:mo>^</m:mo>
</m:mover>
<m:mo>=</m:mo><m:mo>-</m:mo>
<m:mn>173.51</m:mn>
<m:mo>+</m:mo>
<m:mtext>4.83</m:mtext><m:mi>x</m:mi>
<m:mspace width="20pt"/>
</m:math></item>
<item>The two items at the bottom are 
<m:math>
<m:apply>
  <m:power/>
  <m:ci>r</m:ci>
  <m:cn>2</m:cn>
</m:apply>
</m:math> = .43969 
and 
<m:math><m:mi>r</m:mi></m:math>=.663. 
For now, just note where to find these values; we will discuss them in the next two sections.
<newline/></item></list><list id="eip-763" list-type="enumerated" number-style="arabic" class="stepwise"><title>Graphing the Scatterplot and Regression Line</title><item>We are assuming your X data is already entered in list L1 and your Y data is in list L2</item>
<item>Press 2nd STATPLOT ENTER to use Plot 1</item>
<item>On the input screen for PLOT 1, highlight <emphasis>On</emphasis> and press ENTER</item>
<item>For TYPE: highlight the very first icon which is the scatterplot and press ENTER</item>
<item>Indicate Xlist: L1 and Ylist: L2</item>
<item>For Mark: it does not matter which symbol you highlight.</item>

<item> To graph the best fit line, press the "Y=" key and type the equation -173.51 + 4.83x into equation Y1.  (The X key is immediately left of the STAT key).</item>
<item> Press the ZOOM key and then the number 9 (for menu item "ZoomStat"); the calculator will fit the window to the data and graph the line on the scatterplot</item>
<item>Optional: If you want to change the viewing window, press the WINDOW key.  Enter your desired window using Xmin, Xmax, Ymin, Ymax</item></list></section><section id="eip-620"><title>The Correlation Coefficient</title><para id="eip-137">Besides looking at the scatterplot and seeing that a line seems reasonable, how can you tell if the line is a good predictor? Use the correlation coefficient as another indicator (besides the scatterplot) of the strength of the relationship between <m:math><m:mi>x</m:mi></m:math> and <m:math><m:mi>y</m:mi></m:math>.
<newline/><newline/></para><para id="eip-143">The <emphasis>correlation coefficient, <m:math><m:mi>r</m:mi></m:math>, </emphasis> developed by Karl Pearson in the early 1900s, is numerical and provides a measure of strength and direction of the linear association between the independent variable <m:math><m:mi>x</m:mi></m:math> and the dependent variable <m:math><m:mi>y</m:mi></m:math>. <newline/><newline/></para><para id="eip-908">The correlation coefficient is calculated as
<newline/></para><equation id="eip-977"><m:math display="block">
 <m:mrow>
  <m:mi>r</m:mi><m:mo>=</m:mo><m:mfrac>
   <m:mrow>
    <m:mi>n</m:mi><m:mi>Σ</m:mi><m:mo stretchy="false">(</m:mo><m:mi>x</m:mi><m:mi>y</m:mi><m:mo stretchy="false">)</m:mo><m:mo>−</m:mo><m:mo stretchy="false">(</m:mo><m:mi>Σ</m:mi><m:mi>x</m:mi><m:mo stretchy="false">)</m:mo><m:mo stretchy="false">(</m:mo><m:mi>Σ</m:mi><m:mi>y</m:mi><m:mo stretchy="false">)</m:mo>
   </m:mrow>
   <m:mrow>
    <m:msqrt>
     <m:mrow>
      <m:mrow><m:mo>[</m:mo> <m:mrow>
       <m:mi>n</m:mi><m:mi>Σ</m:mi><m:msup>
        <m:mi>x</m:mi>
        <m:mn>2</m:mn>
       </m:msup>
       <m:mo>−</m:mo><m:msup>
        <m:mrow>
         <m:mo stretchy="false">(</m:mo><m:mi>Σ</m:mi><m:mi>x</m:mi><m:mo stretchy="false">)</m:mo>
        </m:mrow>
        <m:mn>2</m:mn>
       </m:msup>
       
      </m:mrow> <m:mo>]</m:mo></m:mrow><m:mrow><m:mo>[</m:mo> <m:mrow>
       <m:mi>n</m:mi><m:mi>Σ</m:mi><m:msup>
        <m:mi>y</m:mi>
        <m:mn>2</m:mn>
       </m:msup>
       <m:mo>−</m:mo><m:msup>
        <m:mrow>
         <m:mo stretchy="false">(</m:mo><m:mi>Σ</m:mi><m:mi>y</m:mi><m:mo stretchy="false">)</m:mo>
        </m:mrow>
        <m:mn>2</m:mn>
       </m:msup>
       
      </m:mrow> <m:mo>]</m:mo></m:mrow>
     </m:mrow>
    </m:msqrt>
    
   </m:mrow>
  </m:mfrac>
  
 </m:mrow>
</m:math>
</equation><para id="eip-13">where <m:math><m:mi>n</m:mi></m:math> = the number of data points.
<newline/><newline/></para><para id="eip-735">If you suspect, judging from the scatterplot, a linear relationship between <m:math><m:mi>x</m:mi></m:math> and <m:math><m:mi>y</m:mi></m:math>, then <m:math><m:mi>r</m:mi></m:math> can measure how strong the linear relationship is.
<newline/><newline/></para><para id="eip-963"><title>What the value of <m:math><m:mi>r</m:mi></m:math> tells us</title><list id="eip-id1166829935791"><item>The value of <m:math><m:mi>r</m:mi></m:math> is always between –1 and +1: –1 ≤ <m:math><m:mi>r</m:mi></m:math> ≤ 1.</item>
<item>The size of the correlation <m:math><m:mi>r</m:mi></m:math> indicates the strength of the linear relationship between <m:math><m:mi>x</m:mi></m:math> and <m:math><m:mi>y</m:mi></m:math>. Values of <m:math><m:mi>r</m:mi></m:math> close to –1 or to +1 indicate a stronger linear relationship between <m:math><m:mi>x</m:mi></m:math> and <m:math><m:mi>y</m:mi></m:math>.</item>

<item>If <m:math><m:mi>r</m:mi></m:math> = 0 there is absolutely no linear relationship between <m:math><m:mi>x</m:mi></m:math> and <m:math><m:mi>y</m:mi></m:math> <emphasis> (no linear correlation)</emphasis>.</item>
<item>If <m:math><m:mi>r</m:mi></m:math> = 1, there is perfect positive correlation. If <m:math><m:mi>r</m:mi></m:math> = –1, there is perfect negative correlation. In both these cases, all of the original data points lie on a straight line. Of course,in the real world, this will not generally happen.<newline/></item>
</list></para><para id="eip-168"><title>What the SIGN of <m:math><m:mi>r</m:mi></m:math> tells us</title><list id="eip-id1171797152624"><item>A positive value of <m:math><m:mi>r</m:mi></m:math> means that when <m:math><m:mi>x</m:mi></m:math> increases, <m:math><m:mi>y</m:mi></m:math> tends to increase and when <m:math><m:mi>x</m:mi></m:math> decreases, <m:math><m:mi>y</m:mi></m:math> tends to decrease <emphasis>(positive correlation)</emphasis>.</item>
<item>A negative value of <m:math><m:mi>r</m:mi></m:math> means that when <m:math><m:mi>x</m:mi></m:math> increases, <m:math><m:mi>y</m:mi></m:math> tends to decrease and when <m:math><m:mi>x</m:mi></m:math> decreases, <m:math><m:mi>y</m:mi></m:math> tends to increase <emphasis>(negative correlation)</emphasis>.</item>
<item>The sign of <m:math><m:mi>r</m:mi></m:math> is the same as the sign of the slope, <m:math><m:mi>b</m:mi></m:math>, of the best-fit line.<newline/></item>
</list></para><note id="eip-208">Strong correlation does not suggest that <m:math><m:mi>x</m:mi></m:math> causes <m:math><m:mi>y</m:mi></m:math> or <m:math><m:mi>y</m:mi></m:math> causes <m:math><m:mi>x</m:mi></m:math>. We say <emphasis>"correlation does not imply causation."</emphasis>
<newline/></note><note id="eip-187"><figure id="linrgs_facts_pics"><media id="eip-idp8185984" alt="Three scatter plots with lines of best fit. The first scatterplot shows points ascending from the lower left to the upper right. The line of best fit has positive slope. The second scatter plot shows points descending from the upper left to the lower right. The line of best fit has negative slope. The third scatter plot of points form a horizontal pattern. The line of best fit is a horizontal line.">
<image src="../../media/fig-ch12_06_01.jpg" mime-type="image/jpeg" print-width="4.5in" width="450"/>
</media><caption>(a) A scatter plot showing data with a positive correlation. 0 &lt; <emphasis effect="italics">r</emphasis> &lt; 1 (b) A scatter plot showing data with a negative correlation. –1 &lt; <emphasis effect="italics">r</emphasis> &lt; 0 (c) A scatter plot showing data with zero correlation. <emphasis effect="italics">r</emphasis> = 0</caption></figure></note><para id="eip-420">The formula for <m:math><m:mi>r</m:mi></m:math> looks formidable. However, computer spreadsheets, statistical software, and many calculators can quickly calculate <m:math><m:mi>r</m:mi></m:math>. The correlation coefficient <m:math><m:mi>r</m:mi></m:math> is the second to last item in the output screens for the LinReg on the TI-83, TI-83+, or TI-84+ calculator (see previous section for instructions).
<newline/></para></section><section id="eip-507"><title>The Coefficient of Determination</title><para id="eip-641"><emphasis>The variable <m:math><m:mi>r</m:mi></m:math><sup>2</sup> is called the</emphasis> <term>coefficient of determination</term> and is the square of the correlation coefficient, but is usually stated as a percent, rather than in decimal form. It has an interpretation in the context of the data:

<newline/></para><list id="eip-611"><item><m:math>
<m:msup>
<m:mi>r</m:mi>
<m:mn>2</m:mn>
</m:msup>
</m:math>, when expressed as a percent, represents the percent of variation in the dependent (predicted) variable <m:math><m:mi>y</m:mi></m:math> that can be explained by variation in the independent (explanatory) variable <m:math><m:mi>x</m:mi></m:math> using the regression (best-fit) line.</item>
<item>1 – <m:math><m:msup>
 <m:mi>r</m:mi> 
<m:mn>2</m:mn>
</m:msup>
</m:math>, when expressed as a percentage, represents the percent of variation in <m:math><m:mi>y</m:mi></m:math> that is NOT explained by variation in <m:math><m:mi>x</m:mi></m:math> using the regression line. This can be seen as the scattering of the observed data points about the regression line.
<newline/></item>
</list><para id="eip-334">Consider the <link target-id="element-22">third exam/final exam example</link> introduced previously 
<newline/>
<list id="eip-624" list-type="bulleted"><item>The line of best fit is: <m:math><m:mover accent="true"><m:mi>y</m:mi><m:mo>^</m:mo></m:mover></m:math> = –173.51 + 4.83<m:math><m:mi>x</m:mi></m:math></item>
 <item>The correlation coefficient is <m:math><m:mi>r</m:mi></m:math> = 0.6631</item>
<item>The coefficient of determination is <m:math><m:mi>r</m:mi></m:math><sup>2</sup> = 0.6631<sup>2</sup> = 0.4397</item>

<item><emphasis>Interpretation of <m:math><m:mi>r</m:mi></m:math><sup>2</sup> in the context of this example:</emphasis></item>
<item>Approximately 44% of the variation (0.4397 is approximately 0.44) in the final-exam grades can be explained by the variation in the grades on the third exam, using the best-fit regression line.</item>  
<item>Therefore, approximately 56% of the variation (1 – 0.44 = 0.56) in the final exam grades can NOT be explained by the variation in the grades on the third exam, using the best-fit regression line. (This is seen as the scattering of the points about the line.)</item>
</list></para></section><section id="eip-83"><title>Section Review</title><para id="eip-35">A regression line, or a line of best fit, can be drawn on a scatterplot and used to predict outcomes for the <m:math><m:mi>x</m:mi></m:math> and <m:math><m:mi>y</m:mi></m:math> variables in a given data set or sample data. There are several ways to find a regression line, but usually the least-squares regression line is used because it creates a uniform line. Residuals, also called “errors,” measure the distance from the actual value of <m:math><m:mi>y</m:mi></m:math> and the estimated value of <m:math><m:mi>y</m:mi></m:math>. The Sum of Squared Errors, when set to its minimum, calculates the points on the line of best fit. Regression lines can be used to predict values within the given set of data, but should not be used to make predictions for values outside the set of data.
<newline/><newline/></para><para id="eip-313">The correlation coefficient <m:math><m:mi>r</m:mi></m:math> measures the strength of the linear association between <m:math><m:mi>x</m:mi></m:math> and <m:math><m:mi>y</m:mi></m:math>. The variable <m:math><m:mi>r</m:mi></m:math> has to be between –1 and +1. When <m:math><m:mi>r</m:mi></m:math> is positive, the <m:math><m:mi>x</m:mi></m:math> and <m:math><m:mi>y</m:mi></m:math> will tend to increase and decrease together. When <m:math><m:mi>r</m:mi></m:math> is negative, <m:math><m:mi>x</m:mi></m:math> will increase and <m:math><m:mi>y</m:mi></m:math> will decrease, or the opposite, <m:math><m:mi>x</m:mi></m:math> will decrease and <m:math><m:mi>y</m:mi></m:math> will increase. The coefficient of determination <m:math><m:mi>r</m:mi></m:math><sup>2</sup>, is equal to the square of the correlation coefficient. When expressed as a percent, <m:math><m:mi>r</m:mi></m:math><sup>2</sup> represents the percent of variation in the dependent variable <m:math><m:mi>y</m:mi></m:math> that can be explained by variation in the independent variable <m:math><m:mi>x</m:mi></m:math> using the regression line.
<newline/><newline/></para></section><section id="eip-581"><title>Practice</title><para id="eip-41"><emphasis effect="italics">Use the following information to answer the next five exercises</emphasis>. A random sample of ten professional athletes produced the following data where <m:math><m:mi>x</m:mi></m:math> is the number of endorsements the player has and <m:math><m:mi>y</m:mi></m:math> is the amount of money made (in millions of dollars).
</para><table id="eip-499" summary="no alt text"><title>Summary</title>
<tgroup cols="4"><colspec colnum="1" colname="c1"/>
<colspec colnum="2" colname="c2"/>
<colspec colnum="3" colname="c3"/>
<colspec colnum="4" colname="c4"/>
<thead>
<row>
<entry><emphasis effect="italics">x</emphasis></entry>
<entry><emphasis effect="italics">y</emphasis></entry>
<entry><emphasis effect="italics">x</emphasis></entry>
<entry><emphasis effect="italics">y</emphasis></entry>
</row>
</thead>
<tbody>
<row>
<entry>0</entry>
<entry>2</entry>
<entry>5</entry>
<entry>12</entry>
</row>
<row>
<entry>3</entry>
<entry>8</entry>
<entry>4</entry>
<entry>9</entry>
</row>
<row>
<entry>2</entry>
<entry>7</entry>
<entry>3</entry>
<entry>9</entry>
</row>
<row>
<entry>1</entry>
<entry>3</entry>
<entry>0</entry>
<entry>3</entry>
</row>
<row>
<entry>5</entry>
<entry>13</entry>
<entry>4</entry>
<entry>10</entry>
</row>
</tbody>



</tgroup>
</table><exercise id="eip-516"><problem id="eip-800">
  <para id="eip-711">Draw a scatterplot of the data.
  </para></problem>

</exercise><exercise id="eip-134"><problem id="eip-690">
  <para id="eip-561">
    Use regression to find the equation for the line of best fit.
  </para>
</problem>

<solution id="eip-542">
  <para id="eip-419"><m:math><m:mover accent="true"><m:mi>y</m:mi><m:mo>^</m:mo></m:mover></m:math> = 2.23 + 1.99<m:math><m:mi>x</m:mi></m:math>
  </para></solution>
</exercise><exercise id="eip-851"><problem id="eip-64">
  <para id="eip-11">
    Draw the line of best fit on the scatterplot.
  </para>
</problem>
</exercise><exercise id="eip-5"><problem id="eip-681">
  <para id="eip-24">
    What is the slope of the line of best fit? What does it represent?
  </para>
</problem>

<solution id="eip-67">
  <para id="eip-441">The slope is 1.99 (<m:math><m:mi>b</m:mi></m:math> = 1.99). It means that for every endorsement deal a professional player gets, he gets an average of another $1.99 million in pay each year.
  </para></solution>
</exercise><exercise id="eip-879"><problem id="eip-820">
  <para id="eip-884">What is the <m:math><m:mi>y</m:mi></m:math>-intercept of the line of best fit? What does it represent?
  </para></problem>

</exercise><exercise id="eip-335"><problem id="eip-105">
  <para id="eip-865">What does an <m:math><m:mi>r</m:mi></m:math> value of zero mean?
  </para></problem>

<solution id="eip-882">
  <para id="eip-292">
    It means that there is no correlation between the data sets.
  </para>
</solution>
</exercise><exercise id="eip-117"><problem id="eip-259">
  <para id="eip-359">When <m:math><m:mi>n</m:mi></m:math> = 100 and <m:math><m:mi>r</m:mi></m:math> = -0.89, is there a significant correlation? Explain.
  </para></problem>

<solution id="eip-997">
  <para id="eip-932">Yes, there are enough data points and the value of <m:math><m:mi>r</m:mi></m:math> is strong enough to show that there is a strong negative correlation between the data sets.
  </para></solution>
</exercise></section><section id="eip-114" class="free-response"><title>Homework</title><exercise id="eip-547"><problem id="eip-663">
  <para id="eip-950">What is the process through which we can calculate a line that goes through a scatterplot with a linear pattern?
  </para></problem>

</exercise><exercise id="eip-122"><problem id="eip-463">
  <para id="eip-387">Explain what it means when a correlation has an <m:math><m:mi>r</m:mi></m:math><sup>2</sup> of 0.72.
  </para></problem>

<solution id="eip-236">
  <para id="eip-780">It means that 72% of the variation in the dependent variable (<m:math><m:mi>y</m:mi></m:math>) can be explained by the variation in the independent variable (<m:math><m:mi>x</m:mi></m:math>).
  </para></solution>
</exercise><exercise id="eip-529"><problem id="eip-508">
  <para id="eip-140">
    Can a coefficient of determination be negative? Why or why not?
  </para>
</problem>
</exercise></section></content>
<glossary>
<definition id="fs-idm1267607808"><term>Line of Best Fit or Least Squares Regression Line</term><meaning id="fs-idm980548400">the straight line which best fits a set of data points on a scatterplot by minimizing the sum of the squared errors (SSE).</meaning></definition>
<definition id="fs-idm1063552320"><term>Linear Regression</term><meaning id="fs-idm929747904">the process of finding the line of best fit or least squares regression line.</meaning></definition>
<definition id="coeffcorr">
    <term>Coefficient of Correlation</term>
    <meaning id="id1167925021109">a measure developed by Karl Pearson (early 1900s) that gives the strength of association between the independent variable and the dependent variable; the formula is:
    <equation id="id5499555"><m:math>
 <m:mrow>
  <m:mi>r</m:mi><m:mo>=</m:mo><m:mfrac>
   <m:mrow>
    <m:mi>n</m:mi><m:msup>
     <m:mstyle mathsize="140%" displaystyle="true"><m:mo>∑</m:mo></m:mstyle>
     <m:mtext>​</m:mtext>
    </m:msup>
    <m:mi>x</m:mi><m:mi>y</m:mi><m:mo>−</m:mo><m:mo stretchy="false">(</m:mo><m:msup>
     <m:mstyle mathsize="140%" displaystyle="true"><m:mo>∑</m:mo></m:mstyle>
     <m:mtext>​</m:mtext>
    </m:msup>
    <m:mi>x</m:mi><m:mo stretchy="false">)</m:mo><m:mo stretchy="false">(</m:mo><m:msup>
     <m:mstyle mathsize="140%" displaystyle="true"><m:mo>∑</m:mo></m:mstyle>
     <m:mtext>​</m:mtext>
    </m:msup>
    <m:mi>y</m:mi><m:mo stretchy="false">)</m:mo>
   </m:mrow>
   <m:mrow>
    <m:msqrt>
     <m:mrow>
      <m:mo stretchy="false">[</m:mo><m:mi>n</m:mi><m:msup>
       <m:mstyle mathsize="140%" displaystyle="true"><m:mo>∑</m:mo></m:mstyle>
       <m:mtext>​</m:mtext>
      </m:msup>
      <m:msup>
       <m:mi>x</m:mi>
       <m:mn>2</m:mn>
      </m:msup>
      <m:mo>−</m:mo><m:msup>
       <m:mrow>
        <m:mo stretchy="false">(</m:mo><m:msup>
         <m:mstyle mathsize="140%" displaystyle="true"><m:mo>∑</m:mo></m:mstyle>
         <m:mtext>​</m:mtext>
        </m:msup>
        <m:mi>x</m:mi><m:mo stretchy="false">)</m:mo>
       </m:mrow>
       <m:mn>2</m:mn>
      </m:msup>
      <m:mo stretchy="false">]</m:mo><m:mo stretchy="false">[</m:mo><m:mi>n</m:mi><m:msup>
       <m:mstyle mathsize="140%" displaystyle="true"><m:mo>∑</m:mo></m:mstyle>
       <m:mtext>​</m:mtext>
      </m:msup>
      <m:msup>
       <m:mi>y</m:mi>
       <m:mn>2</m:mn>
      </m:msup>
      <m:mo>−</m:mo><m:msup>
       <m:mrow>
        <m:mo stretchy="false">(</m:mo><m:msup>
         <m:mstyle mathsize="140%" displaystyle="true"><m:mo>∑</m:mo></m:mstyle>
         <m:mtext>​</m:mtext>
        </m:msup>
        <m:mi>y</m:mi><m:mo stretchy="false">)</m:mo>
       </m:mrow>
       <m:mn>2</m:mn>
      </m:msup>
      <m:mo stretchy="false">]</m:mo>
     </m:mrow>
    </m:msqrt>
    
   </m:mrow>
  </m:mfrac>
  
 </m:mrow>
</m:math></equation> where <m:math><m:mi>n</m:mi></m:math> is the number of data points. The coefficient cannot be more then 1 and less then –1. The closer the coefficient is to ±1, the stronger the evidence of a significant linear relationship between <m:math><m:mi>x</m:mi></m:math> and <m:math><m:mi>y</m:mi></m:math>.
    </meaning>
  
</definition>
<definition id="fs-idm1194860480"><term>Coefficient of Determination</term><meaning id="fs-idm1180103024">the square of the correlation coefficient, usually stated as a percent, and represents the percent of variation in the dependent (predicted) variable <m:math><m:mi>y</m:mi></m:math> that can be explained by variation in the independent (explanatory) variable <m:math><m:mi>x</m:mi></m:math> using the regression (best-fit) line.</meaning></definition>
</glossary>
</document>